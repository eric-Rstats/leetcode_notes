

[TOC]

# 攻破秋招面试盲点

## 1. SQL类问题/Hive

+ [x] 内连接与外链接的区别：

      inner join 是取交集，left （outer）join是以左边为基准，cross join是求笛卡尔积; 
      另外在Hive中只有join, 没有inner join；可以指定哪一个表是最大的表， 
      select /*+stream table(s)*/

+ [x] Map-reduce的原理

      分而治之 -> map完之后需要进行reduce

+ [x] Hive中数据倾斜的原因？解决方案是什么？

      原因：

      数据分布不均匀，各个map阶段数据量差异过大。取决于上层的reduce输出的大小，比如每一个分区的排序结果，若某一个分区出来的排序序列过多，则这一步可能会耗时很多。

      JOIN: 其中一表较小，但是key集中 -> 某一个或某几个reduce上的数据远远高于平均值

      大表与大表，但是分桶的判断字符0值或空值过多。-> 这些空值都由一个reduce处理

      GROUP BY: 维度较小，某值的数量过多  -> 导致某值的reduce非常耗时

      COUNT DISTINCT：某特殊值过多 -> 处理此特殊值的reduce耗时

      ​

      解决方案：

      1.参数调节：hive.map.aggr=true 等

      2.SQL语句

      Join: 小的先进内存，在map端完成reduce

      空值的key变为一个字符串加上随机数，把倾斜的数据分到不同的reduce上。

      将倾斜的数据单独拿出来处理，最后union回去；

      ​

## 2. 机器学习各类算法

### 2.1 Logistic regression

#### i) 原理:

通过$Y=X^T\beta$来拟合条件概率$P(y=1|X)$，

本来是需要考虑h为一个阶跃函数，当$z=X^T\beta < 0 $，则令y=0，否则令y=1；可是由于阶跃函数是不可微的，因此考虑使用比值比:

$$P(Y=1|X)=\frac{1}{1+e^{-X^T\beta}}=1-P(Y=0|X)$$

因此，有
$$ln \frac{\pi}{1-\pi}=z=X^T\beta$$

似然函数为
$$L=\prod_i \pi_i^{y_i}{(1-\pi_i)}^{1-y_i}$$

对数似然函数为:
$$
ln L = \sum_i [y_iln(\pi_i) + (1-y_i)ln(1-\pi_i)] \\ = \sum_i [y_iln(\frac{\pi_i}{1-\pi_i}) + ln(1-\pi_i)] \\ = \sum_i [y_i\beta^T X^{(i)}-ln(1+exp(\beta^T X^{(i)}))]
$$
求极大值(梯度下降法) :
$$\mbox{每一步更新的过程}：\beta_{n+1} = \beta_n - \eta \frac{\delta L}{\delta\beta}$$

若使用牛顿法的话:

$$\mbox{每一步更新的过程}：\beta_{n+1} = \beta_n - (\frac{\partial l}{\partial \beta \partial \beta^T})^{-1} \frac{\delta L}{\delta\beta}$$

用==矩阵的形式==表示 对数似然 如下：
$$
lnL = y^TX\beta- 1_n^Tln(e^{X\beta}+1)
$$
一、二阶导数分别为：
$$
\frac{\partial lnL }{\partial \beta}=X^Ty-X^TP, \mbox{其中P表示成功判为1的概率矩阵} \\
\frac{\partial ^2 lnL }{\partial \beta \partial \beta^T} = - X^TWX, \mbox{W的对角线是$p_i(1-p_i)$}
$$
则牛顿法有：
$$
\beta^{new}=\beta^{old}+(X^TWX)^{-1}X^T(y-p)\\=(X^TWX)^{-1}X^TW(X\beta^{old}+W^{-1}(y-P))\\=(X^TWX)^{-1}X^TWz
\\
\mbox{则即牛顿法每次在迭代求解},\beta^{new} \leftarrow  arg \underset{\beta}{min}(z-X\beta)W(z-X\beta)
$$


**损失函数**
logistic的损失函数为对数损失函数(对数似然损失函数)，即$$L(Y,P(Y|X))=-logP(Y|X)$$,即同样可以推出，等价于对数似然函数
$$-ylog\pi-(1-y)log(1-\pi)$$,最小化，等价于上面的似然函数最大化

#### ii) 为什么要使用sigmoid函数

+ 伯努利分布是指数族分布的一种

​        由（5）推导式子可知，伯努利分布是指数族分布的一种，对于指数族分布的似然函数，可以通过式子求出连接函数，这里得出的结论是使用 比值比 ？ (貌似林建忠的书上有，待细细考究)；

+ 最大熵原理

  在所有的概率模型中，熵最大的模型是最好的模型。

  > 在没有更多信息的情况下，那些不确定的部分都是等可能的，这样熵最大。

最大熵模型中的对偶函数极大化 等价于 最大熵模型的极大似然估计；
$$
P_w(y|x)=\frac{exp(\sum_i^n w_if_i(x,y))}{\sum_yexp(\sum_i^n w_if_i(x,y))}
$$
使用sigmoid函数正是实现了最大熵原理。

#### iii) 为什么要把特征离散化

> 在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：
>
> 0. 离散特征的增加和减少都很容易，易于模型的快速迭代；
> 1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
> 2. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
> 3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
> 4. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
> 5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
> 6. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
>
> 李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。



==关于离散特征交叉，连续型特征离散化==

+ 离散化：one-hot encoding;这样特征数目也得到了上升
+ 特征交叉：如一个特征处本来为1表示，之后可以使用0.5表示这个特征下的广告CTR。
+ 连续特征离散化：根据区间进行划分

> [参考网上的下段话](http://blog.csdn.net/lujiandong1/article/details/52412123)
>
> + 等频离散化需要对原有的每个特征都做，也就是原来的编号为1到13的编号，会离散化成很多的编号，如果每个特征离散化成10个，则最终会有130个特征，训练的结果w就会是一个130维的向量，分别对应着130个特征的权重。
> + 实际的应用表明，离散化的特征能拟合数据中的非线性关系，取得比原有的连续特征更好的效果，而且在线上应用时，无需做乘法运算，也加快了计算ctr的速度。

### 2.2 决策树(基本)

#### i) 原理

**特征选择(划分依据)**

**信息增益**

+ 信息熵

假设一个样本集合D中有属于多个类别的样本，则当前集合的信息熵，即表示混乱程度：
$$entropy = ent(D) = -\sum_ip_ilogp_i,\mbox{每一个i表示一个类}$$

它的值越小，表示纯度越高。

+ 信息增益:用某一个特征进行分裂后纯度的提升

按照某一个特征进行划分后，信息熵减少的程度;

    信息增益 = D的信息熵 - (D|x)的信息熵

其中D|x在x这个变量下分裂后的条件信息熵为每一个子节点的加权平均(按照样本比例)，则信息增益为：

$$Gain(D,a) = Ent(D)-\sum_{v=1}^V\frac{D^v}{D}Ent(D^v)$$

+ 例子：ID3

**信息增益比**

+ 信息增益的缺点： 倾向于选择取数数目较多的属性。

倘若使用编号进行分裂，每一个节点，一个样本，信息增益肯定非常大，但是这个决策树不具有泛化能力。

+ 信息增益比
  $$Gain_{ratio}(D,a)=\frac{Gain(D,a)}{IV(a)}$$
  其中$$IV(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}log\frac{|D^v|}{|D|}$$
  称为属性a的固有值，a的可能取值越多，则IV(a)的值通常会越大。

+ 缺点：增益率对取数数目较少的属性有所偏好。

+ 例子：C4.5(非直接用信息增益率，用了一个启发式的方法，先找到信息增益高于平均水平的属性，再从中选择增益率最高的)

**基尼指数**

CART中的划分规则

$$Gini(D)=1-\sum_{k=1}^{|y|}p_k^2$$
Gini系数反应了类别标记不一致的概率，因此越小表示数据集D的纯度越高。
属性a的基尼指数定义为：
$$Gini(D,a)=\sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)$$

每次选取使得gini指数最小的，作为最优划分属性。

**决策树的生成(深度优先)**

流程：

```
输入：训练数据D，特征集A，阈值e
输出：决策树T

1. if D中所有样本属于同一类别C then 将node标记为C类叶节点, return T
2. if A是空集，或者D中样本在A上取值都相同，则node标记为叶节点，类别标记为D中样本数最多的类，return T
3. 否则，选择最优划分属性a
4.如果信息增益少于阈值e，置其为单节点；否则，按照属性a进行划分
5.剔除属性a之后，对子树递归调用1-4
```

**决策树剪枝**

+ 预剪枝：在生成过程中，每次进行评价看是否能带来泛化性能的提升，不行就停止划分并将当前结点标记为叶节点。
  + 缺点：当前分裂虽然可能导致泛化能力下降，但之后的后续划分可能导致性能显著提高。
+ 后剪枝：生成完后再剪枝，向上搜索看以某一个内部结点为叶节点的子树是不是有更好的泛化能力。
  + 优点：欠拟合风险较小
  + 缺点：在生成完之后再进行，时间等开销比较大

**损失函数**

设树的叶结点个数为|T|,t是树T的叶结点，该叶结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$，$H_t(T)$为叶节点t上的经验熵
$$C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|$$

从叶结点出发，往上面搜寻，若剪枝能带来损失函数的降低，则剪枝。

**CART剪枝**

+ 剪枝，形成一个子树序列
  从整体树$T_0$进行剪枝，对$T_0$的任意内部结点t，以t为单节点的损失函数为:
  $$C_{\alpha}(t)=C(t)+\alpha$$
  以t为根节点的子树$T_t$的损失函数为:
  $$C_{\alpha}(T_t)=C(T_t)+\alpha|T_t|$$

随着 $\alpha$的增大，临界点处，$T_t$与t的损失函数相当，因此需要对$T_t$进行剪枝


for t in $T_0$的所有内部结点:

   计算 $g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}$
求出$g(t)$最小，即最小的一个$\alpha$，$T_0$减去最小的$T_t$，得到的树变为$T_1$，$\alpha_1=g(t)$;$T_1$是区间$[\alpha_1,\alpha_2)$的最优子树。

+ 从这些子树序列中，交叉验证得到最优子树$T_{\alpha}$

上面产生的子树序列，交叉验证得到最优的子树。

**连续值(离散化)**

如C4.5一样，使用二分法将其离散化,由于$[a_i,a_{i+1}]$区间内的任何一个划分点不影响结果，因此划分点一般是两个样本点的中位点。

选取最优的划分点(训练样本两个值的中点)，最优的属性

**缺失值处理**

见周志华(86)

**多变量决策树**

特征对应于一个维度，多个特征其实就决定了高维空间中的分类边界，值得注意的是，这些分类边界都是轴平行的。如何使得分类边界是曲线的呢？

多变量决策树，也称作斜划分。每个非叶结点，都是形如 $\sum_i w_ia_i=t$的线性分类器

### 2.3 集成学习 (随机森林，boosting, GBDT, XGboost)

#### i) 想法

每个个体学习器有着不差的效果，同时之间存在多样性。好而不同



+ 假设：弱分类器（准确率>0.5）, 各分类器之间独立
+ Hoeffding不等式，随着分类器数目T增大，集成的错误率将指数级下降



+ 流派：
  + 强依赖关系：串行生成的序列化方法，boosting
  + 各个分类器之间弱相关：随机森林



#### ii) Boosting 

##### AdaBoost

每一轮训练完之后，更改训练样本数据的权值，将这些弱分类器组合成一个强分类器。

*1.流程：*

样本训练集$T={(x_1,y_1),(x_2,y_2),\ldots, (x_N,y_N)}$

(1) 初始化样本权值分布 为 均匀分布，即每一个样本 $D_1 = (w_{11}\ldots,i=1,\ldots, N), w_{1i}=\frac{1}{N}$

(2) for i in 1: M ,训练M个弱分类器

+ 第m轮 训练弱分类器 $G_m(X)$

+ 第m轮误差 $e_m = \sum_{i=1}^{N} w_{mi}I(G_m(x_i) \neq y_i)$

+ 计算第m个分类器的权重 $\alpha_m = \frac{1}{2}ln\frac{e_m}{1-e_m}$ (则误差越小，最后的发言权更大)

+ 更新训练数据集的权值分布 ： 
  $$
  w_{m+1,i} = \frac{w_{mi} exp{(-\alpha_my_iG_m(x_i))}}{\sum_i
  exp{(-\alpha_my_iG_m(x_i))}} 
  且事实上有:\\
  当G_m(x_i)=y_i即正确分类时，=\frac{w_{mi}}{Z_m}e^{-\alpha_m}\\
  当G_m(x_i)=y_i即错误分类时，=\frac{w_{mi}}{Z_m}e^{\alpha_m}
  $$




(3) 组合： $\hat{f(x)} = \sum_{m=1}^M \alpha_mG_m(x)$

(4) 最终分类器为 $sign(\hat{f}(x))$

==错判的样本权重之和 = $e_m$，$e_m$误差决定分类器发言权重，权重影响下一步的样本权值分布:==

 $$D_m -> e_m -> D_{m+1}$$



*2. 总结*

在训练样本权值更新的时候，错误分类的样本的权值被放大 ，它们在下一轮的训练中起更大作用。$\alpha_m$之和并不为0.



*3.分析*

Adaboost是以指数级别下降的，证明见李航



*4. 算法解释*

可以理解为 加法模型，损失函数为指数函数，学习算法为向前分布算法的 二分类模型。

+ 前向分布算法：每次只学习加法模型的一个基函数

  > (1) 初始化 $f_0=0$
  >
  > (2) for m in 1:M
  >
  > + minimize lost function: $\sum_i L(y_i, f_{m-1}(x_i)+\beta b(x_i,\eta))$ ,得到参数 $\beta_m,\eta_m$
  > + 更新 $f = f_{m-1} + \beta_mb(x_i,\eta_m)$
  >
  > (3) 得到最终的f

  AdaBoost是前向分布算法的一个特例，证明见下：



+ 指数损失函数下，加法前向分布模型
  $$
  L(\hat{f}|D) = E_D exp(-y_i \hat{f_i})   D表示数据集\\
  = e^{-\alpha_t }P(y_i = h_t(x_i)) + e^{\alpha_t h_t}P(y_i \neq h_t(x_i))\\
            = e^{\alpha_t}(1-e_t)+e^{\alpha_t}e_t \\
  此处由于 第t个分类器的误差为 e_t = \sum_i w_{ti}I(y_i \neq h_t(x_i)) 只由误判的比例决定
  $$



$$
则关于 \alpha_t 对L求偏导数，求解得到了每次为啥AdaBoost要这么更新\alpha_t
\\ \alpha_t = \frac{1}{2}ln(\frac{1-e_t}{e_t})
$$

那么基函数每次迭代怎么选择最优，使得损失函数最小呢？（证明来源于周志华P175）
$$
如上面(24)处， L(f_{t-1}+\alpha_t G_t(x)|D)=\sum_{i=1}^N[exp (-y_i (f_{t-1}(x_i)+\alpha_tG_t(x_i)))]\\
G_t(x) = arg\underset{G}{min}L 则等价于下面的式子
\\=arg\underset{G}{min}\sum_{i=1}^Nexp(-y_iG(x_i))
\\考虑泰勒展开，上式 = arg\underset{G}{min}\sum_{i=1}^N(1-y_iG_t(x_i)+\frac{y_i^2 G_t(x_i)^2}{2})\\
由于这里假设是二分类，都是+1或者-1， 有y^2=1,则上式 = arg\underset{G}{min}\sum_{i=1}^N (1-y_iG_t(x_i)+\frac{1}{2})\\
=arg\underset{G}{max}\sum_{i=1}^Ny_iG_t(x_i)=arg\underset{G}{max}\sum_{i=1}^N[1-2I(y_i \neq g_t(x_i))]\\
= arg\underset{G}{min}\sum_{i=1}^NI(y_i \neq G(x_i))
\\ 因此每一步都是选取最小化错判误差的基函数；同时，每一步迭代过程中，样本更新正好与这些式子对应
$$
==因此AdaBoost是GBDT前向分布的一种特例==

==每次训练的时候只是改变样本的权值，如果某一步不满足准确率》0.5，则丢弃，训练过程也会终止==



#### iii) 梯度提升树

boosting若使用平方损失或者对数损失，每一步优化是很简单的，但若是一般的损失函数？

**回归树：**

> (1) 初始化 $f_0(x) = arg \underset{c}{min}\sum_iL(y_i, c)$, 一个根节点，即第一步使用最小化loss function的常数c来训练；
>
> (2) for m in 1:M
>
> + 计算在$f_{m-1}$处的梯度， $-[\frac{\partial L(y_i, f(x_i)}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}$
>
> + 对这个负梯度拟合一个回归树，即负梯度作为残差的一个估计(当loss为平方损失时，即为残差)；
>
>   得到第m个树的叶节点区域$R_{mj}$，对所有j，依旧是寻找常数作为每一个叶节点的代表
>
>   $$C_{mj}=arg\underset{c}{min}\sum_{x \in R_{mj}}L(y_i, f_{m-1}(x)+c)$$
>
> + 更新$f_m(x) = f_{m-1}(x) + \sum_{j=1}^J c_{mj}I(x \in R_{mj})$
>
> (3) 最终 $\hat{f}=\sum_mf_{m}(x)$



**注意**：

传统boosting如果损失函数为square，则每一步对残差再进行更新，其实就好比在上一步的f上加上现今的c；

而如果是一般的可微的损失函数，则这一步负梯度其实是一个估计。



**分类树**：



#### IV) Adaboost怎么推广到多分类呀？



## 3.数据结构与算法

### 3.1 数据结构概览

+ 逻辑结构
+ 存储结构：顺序存储，
  + 链式存储：非连续单元存储，需要指针

## 4.编程语言类

## 5.个人简历中项目问题

## 6. 宝洁八大问等HR类问题

## 7. 最优化、线性代数、矩阵等问题

+ 正定矩阵的充要条件：行列式|A|的顺序主子式全部大于0

### 7.1梯度下降

