[TOC]

此目录主要记录追溯机器算法的过程，同时学习sklearn的过程；

+ 以下部分从《统计学习方法》中第一章，总结一些基础概念。

+ 作为此文件夹的目录

  ​

# 1.统计学习

## 统计学习三要素以及流程

更准确地说，监督学习可以分为三个步骤：



```flow
st=>start: 训练集(假设满足独立同分布)
e=>end: End
op1=>operation: 模型(假设空间使用一类模型，只是参数不同)，
类似于测度论中的事件域
op2=>operation: 策略(评价准侧，最小化mse等，即metric)
op3=>operation: 算法(求解的方法，梯度下降等)
sub1=>subroutine: My Subroutine
cond=>condition: Yes or No?
io=>inputoutput: catch something...
st->op1->op2->op3
```
+ 模型

+ (策略)评价准则

  + 损失函数

    Lost function的期望 $R=E_p(L(Y,f(X)))$ ,可以称为风险函数或期望损失；学习的目标就是选取使得风险最小的模型。

    由于不知道联合分布，却又要去学习条件联合分布，就只能从数据中出发。因此只能利用中心极限定理，用经验风险来逼近期望风险。

    $$R_{emp}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i, f(x_i))$$

  + 经验风险最小化与结构风险最小化

    + 当样本量足够大的时候，经验风险最小化可以保证较好的学习效果；

    比如 极大似然估计(MLE)，当损失函数为对数损失函数，经验风险最小化就等价于极大似然估计。

    + 结构风险最小化：

      由于样本量较小，因此可能会出现过拟合；结构风险最小化其实就是正则化。

      $$R_{emp}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i, f(x_i))+\lambda J(f)$$

      将模型的复杂度当做一个惩罚项放入风险函数中；(从贝叶斯的角度看，这等价于一个先验信息)


+ 算法：由于往往不存在解析解，所以往往要用数值计算和最优化的方法来得到数值解。







**监督学习的模式实质**

我们要学习的模型就属于一个假设空间，这个假设空间是所有由输入空间到输出空间的映射的集合。可以分为两种：

+ 条件概率模型:类似于贝叶斯决策，学习出一个条件概率, $P(Y|X)$，之后argmax来得到预测值。
+ 非概率模型: 直接学习一个决策函数$Y=f(x)$



## 交叉验证与泛化能力

训练集，测试集，验证集的划分



### 泛化能力

对未知数据的预测能力如何？

单纯利用一个测试集来评价模型效果的话，由于数据集的有限大小，并不能真实地反映期望风险。

因此会从理论角度来分析泛化能力，即研究泛化误差的概率上界。



## 生成模型与判别模型

+ 生成模型

  由数据学习联合概率分布$P(X,Y)$,再求出条件概率分布$P(Y|X)$;比如朴素贝叶斯，条件随机场

+ 判别模型

  直接学习$f(X)或P(Y|X)$



## 分类指标

TP:正样本判为正

TN: 负样本判为负

FN:正样本判为负

FP: 负样本判为正



精确率(precision)：判为正的准确率

$$P=\frac{TP}{TP+FP}$$

召回率(recall): 

$$R=\frac{TP}{TP+FN}$$

$$F_1 = \frac{2TP}{2TP+FP+FN},即 \frac{1}{F_1}=\frac{1}{P}+\frac{1}{R}$$



# 2. 具体算法

+ 线性模型
+ KNN
+ logistic
+ LDA

##  