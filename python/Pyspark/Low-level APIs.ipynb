{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Level APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.31.92:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x112965b70>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6,8,10,23]\n",
    "rdd1 = spark.sparkContext.parallelize(a, 2)\n",
    "rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 64, 100, 529]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.map(lambda x: x**2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo\\tStockCode\\tDescription\\tQuantity\\tInvoiceDate\\tUnitPrice\\tCustomerID\\tCountry',\n",
       " '536365\\t85123A\\tWHITE HANGING HEART T-LIGHT HOLDER\\t6\\t2010-12-01 08:26:00\\t2.55\\t17850.0\\tUnited Kingdom',\n",
       " '536365\\t71053\\tWHITE METAL LANTERN\\t6\\t2010-12-01 08:26:00\\t3.39\\t17850.0\\tUnited Kingdom',\n",
       " '536365\\t84406B\\tCREAM CUPID HEARTS COAT HANGER\\t8\\t2010-12-01 08:26:00\\t2.75\\t17850.0\\tUnited Kingdom',\n",
       " '536365\\t84029G\\tKNITTED UNION FLAG HOT WATER BOTTLE\\t6\\t2010-12-01 08:26:00\\t3.39\\t17850.0\\tUnited Kingdom',\n",
       " '536365\\t84029E\\tRED WOOLLY HOTTIE WHITE HEART.\\t6\\t2010-12-01 08:26:00\\t3.39\\t17850.0\\tUnited Kingdom',\n",
       " '536365\\t22752\\tSET 7 BABUSHKA NESTING BOXES\\t2\\t2010-12-01 08:26:00\\t7.65\\t17850.0\\tUnited Kingdom',\n",
       " '536365\\t21730\\tGLASS STAR FROSTED T-LIGHT HOLDER\\t6\\t2010-12-01 08:26:00\\t4.25\\t17850.0\\tUnited Kingdom',\n",
       " '536366\\t22633\\tHAND WARMER UNION JACK\\t6\\t2010-12-01 08:28:00\\t1.85\\t17850.0\\tUnited Kingdom',\n",
       " '536366\\t22632\\tHAND WARMER RED POLKA DOT\\t6\\t2010-12-01 08:28:00\\t1.85\\t17850.0\\tUnited Kingdom']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.sparkContext.textFile('./2010-12-01.csv')\n",
    "df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sparkContext.textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 用python将DataFrame转化为RDD\n",
    "df = spark.range(10)\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[25] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0),\n",
       " Row(id=1),\n",
       " Row(id=2),\n",
       " Row(id=3),\n",
       " Row(id=4),\n",
       " Row(id=5),\n",
       " Row(id=6),\n",
       " Row(id=7),\n",
       " Row(id=8),\n",
       " Row(id=9)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(10).rdd.map(lambda row: row).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycollection = \"Spark the definitive Guide\"\n",
    "words = spark.sparkContext.parallelize(mycollection.split(' '), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "def startsWithS(individual):\n",
    "    return individual.startswith('S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.filter(lambda word: startsWithS(word)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2 = words.map(lambda word: (word, word[0], word.startswith('S')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 'S', True),\n",
       " ('the', 't', False),\n",
       " ('definitive', 'd', False),\n",
       " ('Guide', 'G', False)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 'p',\n",
       " 'a',\n",
       " 'r',\n",
       " 'k',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'i',\n",
       " 'n',\n",
       " 'i',\n",
       " 't',\n",
       " 'i',\n",
       " 'v',\n",
       " 'e',\n",
       " 'G',\n",
       " 'u',\n",
       " 'i',\n",
       " 'd',\n",
       " 'e']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.flatMap(lambda word: list(word)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark', 'the', 'definitive', 'Guide']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['definitive', 'Spark', 'Guide', 'the']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort\n",
    "words.sortBy(lambda word: len(word) * -1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce\n",
    "spark.sparkContext.parallelize(range(1,21)).reduce(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'definitive'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wordLengthReducer(leftword, rightword):\n",
    "    if len(leftword) > len(rightword):\n",
    "        return leftword\n",
    "    else:\n",
    "        return rightword\n",
    "    \n",
    "words.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to files\n",
    "words.saveAsTextFile('./save_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myWords ParallelCollectionRDD[73] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache 可以选择 memory only， disk only 等选项\n",
    "words.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chekpointing save RDD 到磁盘上\n",
    "# 首先要设定sparkcontext存储的路径\n",
    "spark.sparkContext.setCheckpointDir('./')\n",
    "words.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark', 'the', 'definitive', 'Guide']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
    "words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', 1), ('the', 1), ('definitive', 1), ('guide', 1), (':', 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key-value RDDs 最简单的是将其转变为key-value键值对\n",
    "words.map(lambda word: (word.lower(), 1)).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 'Spark'),\n",
       " ('t', 'The'),\n",
       " ('d', 'Definitive'),\n",
       " ('g', 'Guide'),\n",
       " (':', ':'),\n",
       " ('b', 'Big'),\n",
       " ('d', 'Data'),\n",
       " ('p', 'Processing'),\n",
       " ('m', 'Made'),\n",
       " ('s', 'Simple')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过keyby指定一个key\n",
    "keyword = words.keyBy(lambda word: word.lower()[0]) # 使用第一个字母作为key\n",
    "keyword.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 'SPARK'),\n",
       " ('t', 'THE'),\n",
       " ('d', 'DEFINITIVE'),\n",
       " ('g', 'GUIDE'),\n",
       " (':', ':'),\n",
       " ('b', 'BIG'),\n",
       " ('d', 'DATA'),\n",
       " ('p', 'PROCESSING'),\n",
       " ('m', 'MADE'),\n",
       " ('s', 'SIMPLE')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping over values\n",
    "keyword.mapValues(lambda word: word.upper()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 'SPARK'),\n",
       " ('t', 'THE'),\n",
       " ('d', 'DEFINITIVE'),\n",
       " ('g', 'GUIDE'),\n",
       " (':', ':'),\n",
       " ('b', 'BIG'),\n",
       " ('d', 'DATA'),\n",
       " ('p', 'PROCESSING'),\n",
       " ('m', 'MADE'),\n",
       " ('s', 'SIMPLE')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.map(lambda word: (word[0], word[1].upper())).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 'S'),\n",
       " ('s', 'P'),\n",
       " ('s', 'A'),\n",
       " ('s', 'R'),\n",
       " ('s', 'K'),\n",
       " ('t', 'T'),\n",
       " ('t', 'H'),\n",
       " ('t', 'E'),\n",
       " ('d', 'D'),\n",
       " ('d', 'E'),\n",
       " ('d', 'F'),\n",
       " ('d', 'I'),\n",
       " ('d', 'N'),\n",
       " ('d', 'I'),\n",
       " ('d', 'T'),\n",
       " ('d', 'I'),\n",
       " ('d', 'V'),\n",
       " ('d', 'E'),\n",
       " ('g', 'G'),\n",
       " ('g', 'U'),\n",
       " ('g', 'I'),\n",
       " ('g', 'D'),\n",
       " ('g', 'E'),\n",
       " (':', ':'),\n",
       " ('b', 'B'),\n",
       " ('b', 'I'),\n",
       " ('b', 'G'),\n",
       " ('d', 'D'),\n",
       " ('d', 'A'),\n",
       " ('d', 'T'),\n",
       " ('d', 'A'),\n",
       " ('p', 'P'),\n",
       " ('p', 'R'),\n",
       " ('p', 'O'),\n",
       " ('p', 'C'),\n",
       " ('p', 'E'),\n",
       " ('p', 'S'),\n",
       " ('p', 'S'),\n",
       " ('p', 'I'),\n",
       " ('p', 'N'),\n",
       " ('p', 'G'),\n",
       " ('m', 'M'),\n",
       " ('m', 'A'),\n",
       " ('m', 'D'),\n",
       " ('m', 'E'),\n",
       " ('s', 'S'),\n",
       " ('s', 'I'),\n",
       " ('s', 'M'),\n",
       " ('s', 'P'),\n",
       " ('s', 'L'),\n",
       " ('s', 'E')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatmap\n",
    "keyword.flatMapValues(lambda word: word.upper()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 't', 'd', 'g', ':', 'b', 'd', 'p', 'm', 's']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接获取key-value的key或者value\n",
    "keyword.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark',\n",
       " 'The',\n",
       " 'Definitive',\n",
       " 'Guide',\n",
       " ':',\n",
       " 'Big',\n",
       " 'Data',\n",
       " 'Processing',\n",
       " 'Made',\n",
       " 'Simple']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark', 'Simple']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword.lookup('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 'the'), ('t', 'the')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "distinctChars = words.flatMap(lambda word: list(word.lower())).distinct().collect()\n",
    "sampleMap = dict(map(lambda c: (c, random.random()), distinctChars))\n",
    "\n",
    "words.map(lambda word: (word.lower()[0], word))\\\n",
    "    .sampleByKey(True, sampleMap, 6).collect() # withreplacemment=True, fractions=sampleMap, seed=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation\n",
    "chars = words.flatMap(lambda word: word.lower())\n",
    "KVcharacters = chars.map(lambda letter: (letter, 1))\n",
    "\n",
    "def maxFunc(left, right):\n",
    "    return max(left, right)\n",
    "\n",
    "def addFunc(left, right):\n",
    "    return left + right\n",
    "\n",
    "nums = spark.sparkContext.parallelize(range(1,31), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 1),\n",
       " ('p', 1),\n",
       " ('r', 1),\n",
       " ('h', 1),\n",
       " ('d', 2),\n",
       " ('i', 4),\n",
       " ('g', 1),\n",
       " ('a', 1),\n",
       " ('k', 1),\n",
       " ('t', 2),\n",
       " ('e', 4),\n",
       " ('f', 1),\n",
       " ('n', 1),\n",
       " ('v', 1),\n",
       " ('u', 1)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupByKey\n",
    "from functools import reduce\n",
    "KVcharacters.groupByKey().map(lambda row: (row[0], reduce(addFunc, row[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 1),\n",
       " ('p', 1),\n",
       " ('r', 1),\n",
       " ('h', 1),\n",
       " ('d', 2),\n",
       " ('i', 4),\n",
       " ('g', 1),\n",
       " ('a', 1),\n",
       " ('k', 1),\n",
       " ('t', 2),\n",
       " ('e', 4),\n",
       " ('f', 1),\n",
       " ('n', 1),\n",
       " ('v', 1),\n",
       " ('u', 1)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KVcharacters.reduceByKey(addFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 3),\n",
       " ('p', 2),\n",
       " ('r', 1),\n",
       " ('h', 1),\n",
       " ('d', 2),\n",
       " ('i', 4),\n",
       " ('g', 2),\n",
       " ('b', 1),\n",
       " ('c', 1),\n",
       " ('l', 1),\n",
       " ('a', 3),\n",
       " ('k', 1),\n",
       " ('t', 2),\n",
       " ('e', 4),\n",
       " ('f', 1),\n",
       " ('n', 1),\n",
       " ('v', 1),\n",
       " ('u', 1),\n",
       " (':', 1),\n",
       " ('o', 1),\n",
       " ('m', 2)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KVcharacters.aggregateByKey(0, addFunc, maxFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', [1, 1, 1, 1]),\n",
       " ('d', [1, 1, 1, 1]),\n",
       " ('l', [1]),\n",
       " ('v', [1]),\n",
       " (':', [1]),\n",
       " ('p', [1, 1, 1]),\n",
       " ('r', [1, 1]),\n",
       " ('c', [1]),\n",
       " ('k', [1]),\n",
       " ('t', [1, 1, 1]),\n",
       " ('n', [1, 1]),\n",
       " ('u', [1]),\n",
       " ('o', [1]),\n",
       " ('h', [1]),\n",
       " ('i', [1, 1, 1, 1, 1, 1, 1]),\n",
       " ('g', [1, 1, 1]),\n",
       " ('b', [1]),\n",
       " ('a', [1, 1, 1, 1]),\n",
       " ('e', [1, 1, 1, 1, 1, 1, 1]),\n",
       " ('f', [1]),\n",
       " ('m', [1, 1])]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def valToCombiner(value):\n",
    "    return [value]\n",
    "\n",
    "def mergeValuesFunc(vals, valToAppend):\n",
    "    vals.append(valToAppend)\n",
    "    return vals\n",
    "\n",
    "def mergeCoombinerFunc(vals1, vals2):\n",
    "    return vals1 + vals2\n",
    "\n",
    "KVcharacters.combineByKey(valToCombiner, mergeValuesFunc, mergeCoombinerFunc, 6).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 0),\n",
       " ('The', 1),\n",
       " ('Definitive', 2),\n",
       " ('Guide', 3),\n",
       " (':', 4),\n",
       " ('Big', 5),\n",
       " ('Data', 6),\n",
       " ('Processing', 7),\n",
       " ('Made', 8),\n",
       " ('Simple', 9)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRange = spark.sparkContext.parallelize(range(10), 2)\n",
    "words.zip(numRange).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = spark.read.format('csv')\\\n",
    "    .option('header', 'True')\\\n",
    "    .option('inferSchema', 'True')\\\n",
    "    .load('/Users/yanghao/github/data/retail-data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retail.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4298, 4306]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = retail.coalesce(10).rdd\n",
    "\n",
    "def partitionFunc(key):\n",
    "    import random\n",
    "    if key == 17850 and key == 12583:\n",
    "        return 0\n",
    "    else:\n",
    "        return random.randint(1,2)\n",
    "\n",
    "keyedRDD = rdd.keyBy(lambda row: row[6])\n",
    "keyedRDD\\\n",
    "    .partitionBy(3, partitionFunc)\\\n",
    "    .map(lambda x: x[0])\\\n",
    "    .glom()\\\n",
    "    .map(lambda x: len(set(x)))\\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return an RDD created by coalescing all elements within each partition\n",
       "into a list.\n",
       "\n",
       ">>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
       ">>> sorted(rdd.glom().collect())\n",
       "[[1, 2], [3, 4]]\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda/lib/python3.6/site-packages/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd.glom?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast\n",
    "supplementalData = {'Spark': 10000, \"Definitive\": 200,\n",
    "                   \"Big\":-300, \"Simple\":100}\n",
    "suppBroadcast = spark.sparkContext.broadcast(supplementalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Big': -300, 'Definitive': 200, 'Simple': 100, 'Spark': 10000}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suppBroadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Big', -300),\n",
       " ('The', 0),\n",
       " ('Guide', 0),\n",
       " (':', 0),\n",
       " ('Data', 0),\n",
       " ('Processing', 0),\n",
       " ('Made', 0),\n",
       " ('Simple', 100),\n",
       " ('Definitive', 200),\n",
       " ('Spark', 10000)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda word: (word, suppBroadcast.value.get(word, 0)))\\\n",
    "    .sortBy(lambda wordPair: wordPair[1])\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumulator\n",
    "data = spark.read.csv('./2010-12-01.csv', sep='\\t', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "data = data.withColumn('count', lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|count|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|    1|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|    1|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个accumulator\n",
    "accum_variable = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算Country为United Kingdom的个数\n",
    "def func(data):\n",
    "    col1 = data.Country\n",
    "    if col1 == \"United Kingdom\":\n",
    "        accum_variable.add(data['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.foreach(lambda row: func(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
