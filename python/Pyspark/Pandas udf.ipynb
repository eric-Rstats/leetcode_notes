{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Usage Guide for Pandas with Apache Arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanghao/anaconda/lib/python3.6/site-packages/pyarrow/__init__.py:152: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream\n",
      "  warnings.warn(\"pyarrow.open_stream is deprecated, please use \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "如上所示，如果安装了PyArrow，就会使用pyarrow来转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame,\n",
       " pyspark.sql.dataframe.DataFrame,\n",
       " pandas.core.frame.DataFrame)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf), type(df), type(result_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas UDFs\n",
    "\n",
    "### scalar\n",
    "\n",
    "用于将标量的运算向量化，输入是series，输出是相同长度的series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    4\n",
      "2    9\n",
      "dtype: int64\n",
      "+-------------------+\n",
      "|multiply_func(x, x)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  4|\n",
      "|                  9|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# 普通的标量函数\n",
    "def multiply_func(a, b):\n",
    "    return a*b\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=LongType())\n",
    "\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(multiply_func(x, x))\n",
    "\n",
    "# 新建一个spark DataFrame\n",
    "df = spark.createDataFrame(pd.DataFrame(x, columns=['x']))\n",
    "\n",
    "# 执行spark vectorized UDF\n",
    "df.select(multiply(col(\"x\"), col(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|x^2|\n",
      "+---+---+\n",
      "|  1|1.0|\n",
      "|  2|4.0|\n",
      "|  3|9.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('x^2', col(\"x\")**2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Map\n",
    "\n",
    "面向的操作是gruopBy().apply()\n",
    "\n",
    "+ 根据group来split数据\n",
    "+ 对于每个group apply一个函数，输入和输出都是pandas.dataframe\n",
    "+ 将输出combine\n",
    "\n",
    "定义函数的时候需要:\n",
    "+ 定义一个对每一个group 进行运算的函数\n",
    "+ 一个SturctType对象，或者一个字符串，来定义DataFrame的输出的schema信息\n",
    "\n",
    "**注意数据倾斜**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1|-0.5|\n",
      "|  1| 0.5|\n",
      "|  2|-3.0|\n",
      "|  2|-1.0|\n",
      "|  2| 4.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "\n",
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)\n",
    "def subtract_mean(pdf):\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=v-v.mean())\n",
    "\n",
    "df.groupby(\"id\").apply(subtract_mean).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|    v|\n",
      "+---+-----+\n",
      "|  1|  1.0|\n",
      "|  1|  4.0|\n",
      "|  4|  9.0|\n",
      "|  4| 25.0|\n",
      "|  4|100.0|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)\n",
    "def test(pdf):\n",
    "    return pdf.apply(lambda x: x**2)\n",
    "\n",
    "df.groupby(\"id\").apply(test).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Aggregate\n",
    "\n",
    "groupBy().agg 和pyspark.sql.Window\n",
    "\n",
    "将一个或多个pandas.Series转化为scalar.\n",
    "\n",
    "only unbounded window is supported with Grouped aggregate Pandas UDFs currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|mean_udf(v)|\n",
      "+---+-----------+\n",
      "|  1|        1.5|\n",
      "|  2|        6.0|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "\n",
    "@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
    "def mean_udf(v):\n",
    "    return v.mean()\n",
    "\n",
    "df.groupby(\"id\").agg(mean_udf(df[\"v\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|   v|mean_v|\n",
      "+---+----+------+\n",
      "|  1| 1.0|   1.5|\n",
      "|  1| 2.0|   1.5|\n",
      "|  2| 3.0|   6.0|\n",
      "|  2| 5.0|   6.0|\n",
      "|  2|10.0|   6.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window\\\n",
    "    .partitionBy(\"id\")\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "df.withColumn(\"mean_v\", mean_udf(df[\"v\"]).over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持的SQL Types\n",
    "\n",
    "Currently, all Spark SQL data types are supported by Arrow-based conversion except MapType, ArrayType of TimestampType, and nested StructType. BinaryType is supported only when installed PyArrow is equal to or higher then 0.10.0.\n",
    "\n",
    "Note that a standard UDF (non-Pandas) will load timestamp data as Python datetime objects, which is different than a Pandas timestamp. It is recommended to use Pandas time series functionality when working with timestamps in pandas_udfs to get the best performance, see here for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------------------+\n",
      "|id |v  |timestamp              |\n",
      "+---+---+-----------------------+\n",
      "|1  |1.0|2019-07-16 20:29:27.008|\n",
      "|1  |2.0|2019-07-16 20:29:27.008|\n",
      "+---+---+-----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "df = df.withColumn('timestamp', current_timestamp())\n",
    "df.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanghao/anaconda/lib/python3.6/site-packages/pyarrow/__init__.py:152: UserWarning: pyarrow.open_stream is deprecated, please use pyarrow.ipc.open_stream\n",
      "  warnings.warn(\"pyarrow.open_stream is deprecated, please use \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>v</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-07-16 20:29:35.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-07-16 20:29:35.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2019-07-16 20:29:35.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2019-07-16 20:29:35.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2019-07-16 20:29:35.680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     v               timestamp\n",
       "0   1   1.0 2019-07-16 20:29:35.680\n",
       "1   1   2.0 2019-07-16 20:29:35.680\n",
       "2   2   3.0 2019-07-16 20:29:35.680\n",
       "3   2   5.0 2019-07-16 20:29:35.680\n",
       "4   2  10.0 2019-07-16 20:29:35.680"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = df.toPandas()\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    int64\n",
       "v                   float64\n",
       "timestamp    datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
