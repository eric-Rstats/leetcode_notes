{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP examples with Pytorch \n",
    "网址：http://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html\n",
    "\n",
    "An introduction of transfer learning in online tutorial of Pytorch\n",
    "\n",
    "## 一、introduction to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e75e28>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using existing data\n",
    "v_data = [1, 2, 3]\n",
    "V=torch.Tensor(v_data)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  2  3\n",
      " 3  4  5\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# crate a matrix\n",
    "M_data = [[1, 2, 3], [3, 4, 5]]\n",
    "M=torch.Tensor(M_data)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "  1  2\n",
      "  3  4\n",
      "\n",
      "(1 ,.,.) = \n",
      "  5  6\n",
      "  7  8\n",
      "[torch.FloatTensor of size 2x2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3D tensor\n",
    "T_data = [[[1, 2], [3, 4]],\n",
    "               [[5,6], [7, 8]] ]\n",
    "T=torch.Tensor(T_data)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# index into V get a scalar\n",
    "print(V[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# index into M and get a vector\n",
    "print(M[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  2\n",
      " 3  4\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# index into T and get a matrix\n",
    "print(T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认的数据类型是Float,当然可以指定别的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.3996e+14\n",
       "[torch.LongTensor of size 1]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor(1)  # 长整型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      " -2.9718  1.7070 -0.4305 -2.2820  0.5237\n",
      "  0.0004 -1.2039  3.5283  0.4434  0.5848\n",
      "  0.8407  0.5510  0.3863  0.9124 -0.8410\n",
      "  1.2282 -1.8661  1.4146 -1.8781 -0.4674\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.7576  0.4215 -0.4827 -1.1198  0.3056\n",
      "  1.0386  0.5206 -0.5006  1.2182  0.2117\n",
      " -1.0613 -1.9441 -0.9596  0.5489 -0.9901\n",
      " -0.3826  1.5037  1.8267  0.5561  1.6445\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.4973 -1.5067  1.7661 -0.3569 -0.1713\n",
      "  0.4068 -0.4284 -1.1299  1.4274 -1.4027\n",
      "  1.4825 -1.1559  1.6190  0.9581  0.7747\n",
      "  0.1940  0.1687  0.3061  1.0743 -1.0327\n",
      "[torch.FloatTensor of size 3x4x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using random data\n",
    "x = torch.randn((3, 4, 5)) # 3D tensor ; 3个第三维度，4*5每个matrix\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,0 ,.,.) = \n",
       "  1.0930  0.7769 -1.3128  0.7099  0.9944\n",
       " -0.2694 -0.6491 -0.1373 -0.2954 -0.7725\n",
       " -0.2215  0.5074 -0.6794 -1.6115  0.5230\n",
       " -0.8890  0.2620  0.0302  0.0013 -1.3987\n",
       "\n",
       "(0 ,1 ,.,.) = \n",
       "  1.4666 -0.1028 -0.0097 -0.8420 -0.2067\n",
       "  1.0672  0.1732 -0.6873  0.3111  0.2358\n",
       " -1.0658  0.3620  0.3776 -0.2443 -0.5850\n",
       "  2.0812 -0.1186  0.4903  0.8349  0.8894\n",
       "\n",
       "(0 ,2 ,.,.) = \n",
       "  0.4148  0.0507 -0.9644 -2.0111  0.5245\n",
       "  2.1332 -0.0822  0.8388 -1.3233  0.0701\n",
       "  1.2200  0.4251 -1.2328 -0.6195  1.5133\n",
       "  1.9954 -0.6585 -0.4139 -0.2250 -0.6890\n",
       "\n",
       "(0 ,3 ,.,.) = \n",
       "  0.9882  0.7404 -2.0990  1.2582 -0.3990\n",
       " -1.0952 -1.0703  0.6404  1.6199  0.5258\n",
       " -0.2969 -0.0681 -0.2831 -0.4705 -1.7655\n",
       " -0.1656  0.2312 -0.0839 -1.7731 -1.0721\n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       "  1.0248 -0.7116  0.7081  0.8288  1.3526\n",
       "  1.6200  0.3436 -0.9112 -0.9952  0.7455\n",
       "  0.7371  1.2528  0.8503 -0.4165 -0.7499\n",
       "  1.0632  0.0073 -1.4252 -0.0781 -0.5138\n",
       "\n",
       "(1 ,1 ,.,.) = \n",
       "  1.1375 -1.0246 -1.0300 -1.0129  0.0055\n",
       " -0.9347 -0.9882  1.3801 -0.1173  0.9317\n",
       "  1.3267 -1.0173 -1.8575  0.9015  0.1495\n",
       " -0.0336 -0.6076 -1.0048 -0.2826 -0.2711\n",
       "\n",
       "(1 ,2 ,.,.) = \n",
       "  1.3210  1.1608  0.3457 -0.1136 -0.8910\n",
       "  0.2900 -2.1017 -1.1279 -0.8191  0.5334\n",
       "  0.1381  1.6910  1.4114 -0.9804 -0.7578\n",
       " -0.3782  1.7211  0.0310 -0.4270 -0.3868\n",
       "\n",
       "(1 ,3 ,.,.) = \n",
       " -0.6089  1.1652 -0.1326 -0.0228  1.1848\n",
       " -1.0322 -0.7039  0.8813  1.4276 -0.9245\n",
       " -0.8018 -0.7855  0.7877  0.0786  1.7053\n",
       " -0.8098 -0.4594 -1.1798  0.3812 -0.0064\n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       "  0.5302  0.9990  1.0731  1.2506  0.5952\n",
       " -0.2683 -0.3664 -1.0555  0.6623  0.2916\n",
       "  0.0409 -1.1908  0.0167 -0.6838  0.2815\n",
       " -0.1479 -0.2896 -1.1745  0.4368  0.4328\n",
       "\n",
       "(2 ,1 ,.,.) = \n",
       "  0.9969  0.1252  0.4702  0.0781  1.1753\n",
       "  0.2113  0.0110  0.4641  0.5263 -0.6359\n",
       " -0.7046 -0.1754 -2.4768 -0.8536  0.4414\n",
       " -0.6764  0.5185  0.5131 -0.0799  0.4309\n",
       "\n",
       "(2 ,2 ,.,.) = \n",
       " -1.1152 -0.6667  1.0214 -0.1975 -0.8882\n",
       " -0.3583  1.8186  0.2141  0.2588 -0.7857\n",
       "  0.1697  1.5807  0.4838 -1.2901 -0.8039\n",
       "  0.6835  1.0926 -1.2625 -0.0191 -1.0565\n",
       "\n",
       "(2 ,3 ,.,.) = \n",
       " -0.9923 -0.3660  0.6203  0.7167  0.5366\n",
       "  0.2121  1.7014  1.6250  2.1412  0.6155\n",
       "  1.3621  1.0912  0.2104  1.3681 -0.0624\n",
       " -0.2635 -1.3562 -0.5342  0.5256  0.0416\n",
       "[torch.FloatTensor of size 3x4x4x5]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,4,4,5) # 4维度的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations and Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 2\n",
       " 3\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([1., 2., 3.])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 5\n",
       " 7\n",
       " 9\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.Tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7511  0.3649  0.9262 -0.1932 -0.7291\n",
       "-0.3325  0.1134  0.3753 -0.0084  0.5745\n",
       "-0.0230 -0.5933  0.7945  2.2256 -0.8400\n",
       "-0.4712  0.3147 -0.0600  0.9394 -1.3487\n",
       " 0.0537  0.8407 -0.6757  0.0265 -2.0634\n",
       "[torch.FloatTensor of size 5x5]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = torch.randn(2,5)\n",
    "y_1 = torch.randn(3,5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "z_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7511  0.3649  0.9262 -0.1932 -0.7291\n",
       "-0.3325  0.1134  0.3753 -0.0084  0.5745\n",
       "[torch.FloatTensor of size 2x5]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.0230 -0.5933  0.7945  2.2256 -0.8400\n",
       "-0.4712  0.3147 -0.0600  0.9394 -1.3487\n",
       " 0.0537  0.8407 -0.6757  0.0265 -2.0634\n",
       "[torch.FloatTensor of size 3x5]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7511  0.3649  0.9262 -0.1932 -0.7291 -1.9366  1.0067 -1.8593  0.9329  1.4066\n",
       "-0.3325  0.1134  0.3753 -0.0084  0.5745  1.4414  0.1690  0.2575  0.1212 -1.8270\n",
       "[torch.FloatTensor of size 2x10]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵拼接，默认是第0维度，就是行；\n",
    "# 连接列\n",
    "torch.cat([x_1, torch.randn(2,5)], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape Tensor\n",
    "就是重塑张量的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.1571 -1.3312 -1.0505 -1.0007 -0.4621 -0.5060  1.1233  0.4800 -0.0344 -0.4928\n",
       "-0.0653 -2.2272 -0.5412 -0.9734 -0.0499  0.5303  1.5544  0.6882 -0.4737  0.5039\n",
       "\n",
       "Columns 10 to 14 \n",
       "-0.2699 -0.8699  0.8155 -0.6616 -0.2193\n",
       "-0.2694 -0.2859  0.1109  0.4339  0.0103\n",
       "[torch.FloatTensor of size 2x15]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 5)\n",
    "x.view(2, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  0.1571 -1.3312 -1.0505 -1.0007 -0.4621\n",
       " -0.5060  1.1233  0.4800 -0.0344 -0.4928\n",
       " -0.2699 -0.8699  0.8155 -0.6616 -0.2193\n",
       "\n",
       "(1 ,.,.) = \n",
       " -0.0653 -2.2272 -0.5412 -0.9734 -0.0499\n",
       "  0.5303  1.5544  0.6882 -0.4737  0.5039\n",
       " -0.2694 -0.2859  0.1109  0.4339  0.0103\n",
       "[torch.FloatTensor of size 2x3x5]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.1571 -1.3312 -1.0505 -1.0007 -0.4621 -0.5060  1.1233  0.4800 -0.0344 -0.4928\n",
       "-0.2699 -0.8699  0.8155 -0.6616 -0.2193 -0.0653 -2.2272 -0.5412 -0.9734 -0.0499\n",
       " 0.5303  1.5544  0.6882 -0.4737  0.5039 -0.2694 -0.2859  0.1109  0.4339  0.0103\n",
       "[torch.FloatTensor of size 3x10]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果一个维度为-1，它的维度可以被推断出来\n",
    "x.view((3,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算图与automatic Differentiation\n",
    "The Variable class keeps track of how it was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 2\n",
       " 3\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.Tensor([1., 2., 3]), requires_grad=True)\n",
    "x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 5\n",
       " 7\n",
       " 9\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = Variable(torch.Tensor([4., 5., 6]), requires_grad=True)\n",
    "z = x + y\n",
    "z.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd._functions.basic_ops.Add at 0x3aba4d8>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出Variables知道他们是怎么是产生的，即它会记录其creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 21\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = z.sum()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd._functions.reduce.Sum at 0x3aba3f0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2\n",
       " 2\n",
       " 2\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.backward()\n",
    "x.grad     # 梯度会堆积起来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 该如何理解variable可以追踪其Creator呢？其实variable由别的variable计算而来，自己本身就会存储源头的信息和怎么来的计算；\n",
    "而其对应的tensor中则没有相关信息，因此单独地将其data中的Tensor提取出来再存进variable中就会丢失其Creator信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 4\n",
       " 6\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " x = torch.Tensor((2, 3))\n",
    "y = torch.Tensor((2,3))\n",
    "z = x+ y\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 4\n",
       " 6\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_x = Variable(x)\n",
    "var_y = Variable(y)\n",
    "var_z = var_x + var_y\n",
    "var_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd._functions.basic_ops.Add at 0x3aba220>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_z.creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# 如果将var_z的数据提取出来再建立一个新的Variable,就会丢失\n",
    "var_z_data = var_z.data\n",
    "new = Variable(var_z_data)\n",
    "print(new.creator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**因此，我们在code中需要注意不能过多地打断链式法则**\n",
    "\n",
    "## 二、deep learning with Pytorch\n",
    "\n",
    "### Affine Maps 映射\n",
    "$$f(x)=Ax+b$$\n",
    "此处需要进行学习的就是$A$和$b$\n",
    "\n",
    "**注**：Pytorch中处理映射的时候，使用的是行，而不是使用列。ith row of the output is the map of ith row of input and b;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e75e28>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mbox{maps from} R^5 \\mbox{to} R^3$     从维度是5降到维度是3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.3130  0.2576  1.3546\n",
       " 1.0007  0.6433  0.4951\n",
       "-0.3767 -0.1680  0.5511\n",
       "-0.6396 -1.1769 -0.0147\n",
       "-0.4327 -0.2869 -0.1978\n",
       " 0.5218  0.6403  0.6204\n",
       "-1.4568  0.3256 -0.5250\n",
       " 0.3919 -0.5869 -0.1451\n",
       " 0.7158  0.2852 -0.1099\n",
       " 0.2985 -0.3870  0.1984\n",
       "[torch.FloatTensor of size 10x3]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = nn.Linear(5, 3)\n",
    "\n",
    "# 由于框架中是对于行进行操作，可知5维的输入，即要求列有5列；\n",
    "# 即 n * 5 的矩阵映射成 n * 3\n",
    "\n",
    "data = Variable(torch.randn(10, 5))\n",
    "lin(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linearity\n",
    "线性函数堆积起来仍然是线性函数\n",
    "有一些核心的非线性函数  $tanh(x), \\sigma(x), ReLU(x) $,他们的优势是梯度计算起来比较方便\n",
    "\n",
    "在pytorch中线性函数 在pytorch.functional中，他们没有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.2954 -0.7725\n",
      "-0.2215  0.5074\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 0.0000  0.0000\n",
      " 0.0000  0.5074\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Variable(torch.randn(2, 2))\n",
    "print(data)\n",
    "print(F.relu(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax and probabilities\n",
    "softmax的output是一个概率分布；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.6794\n",
      "-1.6115\n",
      " 0.5230\n",
      "-0.8890\n",
      " 0.2620\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      " 0.1235\n",
      " 0.0486\n",
      " 0.4111\n",
      " 0.1002\n",
      " 0.3166\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "-2.0914\n",
      "-3.0235\n",
      "-0.8890\n",
      "-2.3010\n",
      "-1.1500\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Variable(torch.randn(5))\n",
    "print(data)\n",
    "print(F.softmax(data))\n",
    "print(F.softmax(data).sum())\n",
    "print(F.log_softmax(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Funcions\n",
    "### optimization and Training\n",
    "optim的选择不同，可能训练效果不同\n",
    "\n",
    "### Creating Network Components in Pytorch\n",
    "### Example: logistic regression Bag-of-words classifier\n",
    "\n",
    "词袋分类器：假如只有 hello 和 world两个词，则 ''hello hello world hello的向量表示是\n",
    "$$[3, 1]$$,而‘hello hello’的向量表示是:\n",
    "$$[2,0]$$\n",
    "\n",
    "定义此向量为$x$， 则神经网络的output为 $log softmax(Ax+b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['me', 'gusta', 'comer', 'en', 'la', 'cafeteria'], 'SPANISH'),\n",
       " (['Give', 'it', 'to', 'me'], 'ENGLISH'),\n",
       " (['No', 'creo', 'que', 'sea', 'una', 'buena', 'idea'], 'SPANISH'),\n",
       " (['No',\n",
       "   'it',\n",
       "   'is',\n",
       "   'not',\n",
       "   'a',\n",
       "   'good',\n",
       "   'idea',\n",
       "   'to',\n",
       "   'get',\n",
       "   'lost',\n",
       "   'at',\n",
       "   'sea'],\n",
       "  'ENGLISH')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 3, 'No': 9, 'buena': 14, 'it': 7, 'at': 22, 'sea': 12, 'cafeteria': 5, 'Yo': 23, 'la': 4, 'to': 8, 'creo': 10, 'is': 16, 'a': 18, 'good': 19, 'get': 20, 'idea': 15, 'que': 11, 'not': 17, 'me': 0, 'on': 25, 'gusta': 1, 'lost': 21, 'Give': 6, 'una': 13, 'si': 24, 'comer': 2}\n"
     ]
    }
   ],
   "source": [
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "word_to_ix = {}\n",
    "\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word]=len(word_to_ix)\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0984  0.0541  0.0886 -0.1466  0.1503  0.0746  0.0485  0.0580  0.0984 -0.0573\n",
      " 0.1763 -0.1710 -0.0196 -0.0568  0.0307  0.1733 -0.0360 -0.0471 -0.1031  0.1031\n",
      "\n",
      "Columns 10 to 19 \n",
      "-0.0593  0.1032 -0.0902 -0.0563  0.1553  0.0992 -0.0282  0.1496  0.1823 -0.1915\n",
      " 0.1582  0.1065  0.0289 -0.0779 -0.1950  0.1070  0.0459 -0.1361 -0.0680  0.0308\n",
      "\n",
      "Columns 20 to 25 \n",
      " 0.0641 -0.0007  0.0477 -0.1672 -0.1511  0.1126\n",
      " 0.0106 -0.1926  0.1514  0.0820 -0.0560 -0.0115\n",
      "[torch.FloatTensor of size 2x26]\n",
      "\n",
      "Parameter containing:\n",
      " 0.1602\n",
      " 0.1038\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "Variable containing:\n",
      "-0.5790 -0.8220\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is num_labels!\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec))\n",
    "\n",
    "\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])\n",
    "\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "\n",
    "# the model knows its parameters.  The first output below is A, the second is b.\n",
    "# Whenever you assign a component to a class variable in the __init__ function\n",
    "# of a module, which was done with the line\n",
    "# self.linear = nn.Linear(...)\n",
    "# Then through some Python magic from the Pytorch devs, your module\n",
    "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "# To run the model, pass in a BoW vector, but wrapped in an autograd.Variable\n",
    "sample = data[0]\n",
    "bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "log_probs = model(autograd.Variable(bow_vector))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.9794 -0.4708\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-0.5436 -0.8690\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-0.0593\n",
      " 0.1582\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "Variable containing:\n",
      "-0.2212 -1.6174\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-2.6540 -0.0730\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      " 0.3903\n",
      "-0.2913\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}\n",
    "\n",
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "\n",
    "# Print the matrix column corresponding to \"creo\"\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Usually you want to pass over the training data several times.\n",
    "# 100 is much bigger than on a real data set, but real datasets have more than\n",
    "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Make our BOW vector and also we must wrap the target in a\n",
    "        # Variable as an integer. For example, if the target is SPANISH, then\n",
    "        # we wrap the integer 0. The loss function then knows that the 0th\n",
    "        # element of the log probabilities is the log probability\n",
    "        # corresponding to SPANISH\n",
    "        bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "        target = autograd.Variable(make_target(label, label_to_ix))\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        log_probs = model(bow_vec)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "\n",
    "# Index corresponding to Spanish goes up, English goes down!\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
